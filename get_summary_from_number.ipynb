{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<feed xmlns=\"http://www.w3.org/2005/Atom\">\n",
      "  <link href=\"http://arxiv.org/api/query?search_query%3D%26id_list%3D1607.06450%26start%3D0%26max_results%3D1\" rel=\"self\" type=\"application/atom+xml\"/>\n",
      "  <title type=\"html\">ArXiv Query: search_query=&amp;id_list=1607.06450&amp;start=0&amp;max_results=1</title>\n",
      "  <id>http://arxiv.org/api/DrO43Rw0z+qYECyDyJ3Iouk/jQ4</id>\n",
      "  <updated>2023-09-16T00:00:00-04:00</updated>\n",
      "  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">1</opensearch:totalResults>\n",
      "  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\n",
      "  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">1</opensearch:itemsPerPage>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1607.06450v1</id>\n",
      "    <updated>2016-07-21T19:57:52Z</updated>\n",
      "    <published>2016-07-21T19:57:52Z</published>\n",
      "    <title>Layer Normalization</title>\n",
      "    <summary>  Training state-of-the-art, deep neural networks is computationally expensive.\n",
      "One way to reduce the training time is to normalize the activities of the\n",
      "neurons. A recently introduced technique called batch normalization uses the\n",
      "distribution of the summed input to a neuron over a mini-batch of training\n",
      "cases to compute a mean and variance which are then used to normalize the\n",
      "summed input to that neuron on each training case. This significantly reduces\n",
      "the training time in feed-forward neural networks. However, the effect of batch\n",
      "normalization is dependent on the mini-batch size and it is not obvious how to\n",
      "apply it to recurrent neural networks. In this paper, we transpose batch\n",
      "normalization into layer normalization by computing the mean and variance used\n",
      "for normalization from all of the summed inputs to the neurons in a layer on a\n",
      "single training case. Like batch normalization, we also give each neuron its\n",
      "own adaptive bias and gain which are applied after the normalization but before\n",
      "the non-linearity. Unlike batch normalization, layer normalization performs\n",
      "exactly the same computation at training and test times. It is also\n",
      "straightforward to apply to recurrent neural networks by computing the\n",
      "normalization statistics separately at each time step. Layer normalization is\n",
      "very effective at stabilizing the hidden state dynamics in recurrent networks.\n",
      "Empirically, we show that layer normalization can substantially reduce the\n",
      "training time compared with previously published techniques.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Jimmy Lei Ba</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jamie Ryan Kiros</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Geoffrey E. Hinton</name>\n",
      "    </author>\n",
      "    <link href=\"http://arxiv.org/abs/1607.06450v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1607.06450v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "</feed>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import urllib, urllib.request\n",
    "url = 'http://export.arxiv.org/api/query?id_list=1706.03762&search_query=all:attention&start=0&max_results=1'\n",
    "url = 'http://export.arxiv.org/api/query?id_list=1607.06450&start=0&max_results=1'\n",
    "\n",
    "#url = 'http://export.arxiv.org/api/query?id_list=1706.06450&search_query=all&max_results=1'\n",
    "\n",
    "data = urllib.request.urlopen(url)\n",
    "decoded_data = data.read().decode('utf-8')\n",
    "print(decoded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "#  remove every item before <summary> and after </summary>\n",
    "def get_summary(decoded_data):\n",
    "    summary = re.search('<summary>.*?</summary>', decoded_data, re.DOTALL)\n",
    "    if summary is None:\n",
    "        return None\n",
    "    else:\n",
    "        return summary.group(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<feed xmlns=\"http://www.w3.org/2005/Atom\">\n",
      "  <link href=\"http://arxiv.org/api/query?search_query%3D%26id_list%3D1607.06450%26start%3D0%26max_results%3D1\" rel=\"self\" type=\"application/atom+xml\"/>\n",
      "  <title type=\"html\">ArXiv Query: search_query=&amp;id_list=1607.06450&amp;start=0&amp;max_results=1</title>\n",
      "  <id>http://arxiv.org/api/DrO43Rw0z+qYECyDyJ3Iouk/jQ4</id>\n",
      "  <updated>2023-09-16T00:00:00-04:00</updated>\n",
      "  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">1</opensearch:totalResults>\n",
      "  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\n",
      "  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">1</opensearch:itemsPerPage>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1607.06450v1</id>\n",
      "    <updated>2016-07-21T19:57:52Z</updated>\n",
      "    <published>2016-07-21T19:57:52Z</published>\n",
      "    <title>Layer Normalization</title>\n",
      "    <summary>  Training state-of-the-art, deep neural networks is computationally expensive.\n",
      "One way to reduce the training time is to normalize the activities of the\n",
      "neurons. A recently introduced technique called batch normalization uses the\n",
      "distribution of the summed input to a neuron over a mini-batch of training\n",
      "cases to compute a mean and variance which are then used to normalize the\n",
      "summed input to that neuron on each training case. This significantly reduces\n",
      "the training time in feed-forward neural networks. However, the effect of batch\n",
      "normalization is dependent on the mini-batch size and it is not obvious how to\n",
      "apply it to recurrent neural networks. In this paper, we transpose batch\n",
      "normalization into layer normalization by computing the mean and variance used\n",
      "for normalization from all of the summed inputs to the neurons in a layer on a\n",
      "single training case. Like batch normalization, we also give each neuron its\n",
      "own adaptive bias and gain which are applied after the normalization but before\n",
      "the non-linearity. Unlike batch normalization, layer normalization performs\n",
      "exactly the same computation at training and test times. It is also\n",
      "straightforward to apply to recurrent neural networks by computing the\n",
      "normalization statistics separately at each time step. Layer normalization is\n",
      "very effective at stabilizing the hidden state dynamics in recurrent networks.\n",
      "Empirically, we show that layer normalization can substantially reduce the\n",
      "training time compared with previously published techniques.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Jimmy Lei Ba</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jamie Ryan Kiros</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Geoffrey E. Hinton</name>\n",
      "    </author>\n",
      "    <link href=\"http://arxiv.org/abs/1607.06450v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1607.06450v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "</feed>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(decoded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import urllib, urllib.request\n",
    "import re\n",
    "url = 'http://export.arxiv.org/api/query?id_list=1706.03762&search_query=all:attention&start=0&max_results=1'\n",
    "url = 'http://export.arxiv.org/api/query?id_list=1607.06450&start=0&max_results=1'\n",
    "\n",
    "def get_response_arxiv_number(arxiv_number):\n",
    "    url = 'http://export.arxiv.org/api/query?id_list=' + arxiv_number + '&start=0&max_results=1'\n",
    "    data = urllib.request.urlopen(url)\n",
    "    decoded_data = data.read().decode('utf-8')\n",
    "    return decoded_data\n",
    "\n",
    "\n",
    "def get_summary_from_arxiv_number(arxiv_number):\n",
    "    decoded_data = get_response_arxiv_number(arxiv_number)\n",
    "    summary = re.search('<summary>.*?</summary>', decoded_data, re.DOTALL)\n",
    "    if summary is None:\n",
    "        return None\n",
    "    else:\n",
    "        return summary.group(0)\n",
    "\n",
    "# get item between html tag <link title=\"pdf\" href=\"\" ...>\n",
    "def get_pdf_link_from_arxiv_number(arxiv_number):\n",
    "    decoded_data = get_response_arxiv_number(arxiv_number)\n",
    "    pdf_link = re.search('<link title=\"pdf\" href=\".*?\"', decoded_data, re.DOTALL)\n",
    "    #return only the pdf link\n",
    "    if pdf_link is None:\n",
    "        return None\n",
    "    else:\n",
    "        return pdf_link.group(0)[24:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<summary>  The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks in an encoder-decoder configuration. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer, based\n",
      "solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to be\n",
      "superior in quality while being more parallelizable and requiring significantly\n",
      "less time to train. Our model achieves 28.4 BLEU on the WMT 2014\n",
      "English-to-German translation task, improving over the existing best results,\n",
      "including ensembles by over 2 BLEU. On the WMT 2014 English-to-French\n",
      "translation task, our model establishes a new single-model state-of-the-art\n",
      "BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\n",
      "of the training costs of the best models from the literature. We show that the\n",
      "Transformer generalizes well to other tasks by applying it successfully to\n",
      "English constituency parsing both with large and limited training data.\n",
      "</summary>\n",
      "http://arxiv.org/pdf/1706.03762v7\n"
     ]
    }
   ],
   "source": [
    "print(get_summary_from_arxiv_number('1706.03762'))\n",
    "print(get_pdf_link_from_arxiv_number('1706.03762'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def download_html_as_pdf(url, save_as_filename):\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        with open(save_as_filename, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "    else:\n",
    "        print(f\"Failed to fetch {url}. Status code: {response.status_code}\")\n",
    "\n",
    "# Use the function\n",
    "url = get_pdf_link_from_arxiv_number('1706.03762')\n",
    "save_as_filename = \"downloaded_file.html.pdf\"\n",
    "download_html_as_pdf(url, save_as_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pdf_from_arxiv_number(arxiv_number, save_as_filename):\n",
    "    url = get_pdf_link_from_arxiv_number(arxiv_number)\n",
    "    download_html_as_pdf(url, save_as_filename)\n",
    "\n",
    "arxiv_number = '1308.0850'\n",
    "\n",
    "download_pdf_from_arxiv_number(arxiv_number, arxiv_number + '.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
